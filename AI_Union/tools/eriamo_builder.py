#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UNIFIED_GENESIS_FIX.py v4.0 - Kompletna Integracja Genesis

Ten skrypt:
1. Parsuje wszystkie pliki genesis*.py (NIE uÅ¼ywa exec!)
2. Mapuje emocje na 8-wymiarowe wektory Plutchika
3. Åaduje wiedzÄ™ bezpoÅ›rednio do eriamo.soul
4. Naprawia aii.py (threshold, tag matching)

Wektory: [radoÅ›Ä‡, smutek, strach, gniew, miÅ‚oÅ›Ä‡, wstrÄ™t, zaskoczenie, akceptacja]
"""

import json
import sys
import os
import time
import re
from pathlib import Path
from typing import Dict, List, Tuple, Any

class Colors:
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    RESET = '\033[0m'
    MAGENTA = '\033[95m'
    BOLD = '\033[1m'

# ============================================================
# MAPOWANIE EMOCJI NA WEKTORY 8D (Plutchik)
# ============================================================
# Indeksy: [radoÅ›Ä‡, smutek, strach, gniew, miÅ‚oÅ›Ä‡, wstrÄ™t, zaskoczenie, akceptacja]

EMOTION_VECTORS = {
    # Podstawowe emocje (dominanta + wsparcie)
    "radoÅ›Ä‡":      [0.9, 0.0, 0.0, 0.0, 0.3, 0.0, 0.2, 0.4],
    "smutek":      [0.0, 0.9, 0.2, 0.0, 0.3, 0.0, 0.0, 0.2],
    "strach":      [0.0, 0.3, 0.9, 0.1, 0.0, 0.2, 0.3, 0.1],
    "gniew":       [0.0, 0.1, 0.2, 0.9, 0.0, 0.3, 0.1, 0.0],
    "miÅ‚oÅ›Ä‡":      [0.4, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1, 0.5],
    "wstrÄ™t":      [0.0, 0.2, 0.3, 0.2, 0.0, 0.9, 0.1, 0.0],
    "zaskoczenie": [0.3, 0.0, 0.2, 0.0, 0.1, 0.0, 0.9, 0.3],
    "akceptacja":  [0.3, 0.0, 0.0, 0.0, 0.4, 0.0, 0.1, 0.9],
}

# Specjalne wektory dla typÃ³w wiedzy
KNOWLEDGE_TYPE_VECTORS = {
    # Definicje strukturalne (X to Y) - neutralne, akceptacja
    "definicja":   [0.2, 0.0, 0.0, 0.0, 0.2, 0.0, 0.3, 0.8],
    
    # Gramatyka - strukturalna akceptacja
    "gramatyka":   [0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.2, 0.9],
    
    # Matematyka - logiczna akceptacja z zaskoczeniem
    "matematyka":  [0.2, 0.0, 0.0, 0.0, 0.1, 0.0, 0.4, 0.8],
    
    # Pytania - ciekawoÅ›Ä‡ (zaskoczenie + akceptacja)
    "pytanie":     [0.2, 0.0, 0.1, 0.0, 0.2, 0.0, 0.8, 0.5],
    
    # Powitania - radoÅ›Ä‡ + miÅ‚oÅ›Ä‡
    "powitanie":   [0.8, 0.0, 0.0, 0.0, 0.6, 0.0, 0.2, 0.5],
    
    # Aksjomaty - gÅ‚Ä™boka akceptacja
    "aksjomat":    [0.1, 0.0, 0.0, 0.0, 0.3, 0.0, 0.2, 0.95],
    
    # Wspomnienia - mieszanka z dominantÄ… emocji
    "wspomnienie": [0.3, 0.2, 0.1, 0.1, 0.4, 0.0, 0.3, 0.5],
    
    # ToÅ¼samoÅ›Ä‡ EriAmo - miÅ‚oÅ›Ä‡ + akceptacja + zaskoczenie
    "toÅ¼samoÅ›Ä‡":   [0.5, 0.0, 0.0, 0.0, 0.8, 0.0, 0.4, 0.7],
}

def modulate_vector(base_emotion: str, strength: float = 1.0, 
                   secondary: str = None, secondary_strength: float = 0.3) -> List[float]:
    """
    Tworzy wektor emocjonalny z modulacjÄ… siÅ‚y i opcjonalnÄ… emocjÄ… wtÃ³rnÄ….
    """
    vector = EMOTION_VECTORS.get(base_emotion, EMOTION_VECTORS["akceptacja"]).copy()
    
    # Modulacja siÅ‚Ä…
    vector = [v * strength for v in vector]
    
    # Dodaj emocjÄ™ wtÃ³rnÄ…
    if secondary and secondary in EMOTION_VECTORS:
        sec_vec = EMOTION_VECTORS[secondary]
        vector = [v + sec_vec[i] * secondary_strength for i, v in enumerate(vector)]
    
    # Normalizacja do [0, 1]
    max_val = max(vector) if max(vector) > 0 else 1
    vector = [min(v / max_val, 1.0) for v in vector]
    
    return [round(v, 3) for v in vector]


def parse_genesis_data(genesis_dir: str) -> Dict[str, List[Dict]]:
    """
    Parsuje pliki genesis i zwraca ustrukturyzowane dane.
    NIE uÅ¼ywa exec() - bezpieczne parsowanie!
    """
    all_data = {
        "lexicon": [],      # SÅ‚owa do leksykonu
        "memories": [],     # Wspomnienia/definicje
        "axioms": [],       # Aksjomaty
    }
    
    genesis_files = [
        "genesis.py",
        "genesisdef.py",
        "genesis_grammar.py",
        "genesis_math.py",
        "genesiskit.py",
        "genesispyt.py",
        "genesissk.py",
    ]
    
    for filename in genesis_files:
        filepath = os.path.join(genesis_dir, filename)
        if not os.path.exists(filepath):
            continue
            
        print(f"  ðŸ“– Parsowanie: {filename}...")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # WyodrÄ™bnij dane w zaleÅ¼noÅ›ci od typu pliku
        if filename == "genesis.py":
            parse_genesis_main(content, all_data)
        elif filename == "genesisdef.py":
            parse_definitions(content, all_data)
        elif filename == "genesis_grammar.py":
            parse_grammar(content, all_data)
        elif filename == "genesis_math.py":
            parse_math(content, all_data)
        elif filename == "genesiskit.py":
            parse_emotions_kit(content, all_data)
        elif filename == "genesispyt.py":
            parse_questions(content, all_data)
        elif filename == "genesissk.py":
            parse_syntax(content, all_data)
    
    return all_data


def extract_list_items(content: str, list_name: str) -> List[str]:
    """WyodrÄ™bnia elementy z listy Pythona w kodzie."""
    pattern = rf'{list_name}\s*=\s*\[(.*?)\]'
    match = re.search(pattern, content, re.DOTALL)
    if match:
        items_str = match.group(1)
        items = re.findall(r'"([^"]*)"', items_str)
        return items
    return []


def extract_dict_items(content: str, dict_name: str) -> Dict[str, Tuple]:
    """WyodrÄ™bnia elementy ze sÅ‚ownika Pythona w kodzie."""
    pattern = rf'{dict_name}\s*=\s*\{{(.*?)\}}'
    match = re.search(pattern, content, re.DOTALL)
    if match:
        dict_content = match.group(1)
        # Parsuj pary klucz: wartoÅ›Ä‡
        items = {}
        # Uproszczone parsowanie - szukamy wzorca "klucz": (wartoÅ›ci)
        pairs = re.findall(r'"([^"]+)":\s*\(([^)]+)\)', dict_content)
        for key, values in pairs:
            items[key] = values
        return items
    return {}


def parse_genesis_main(content: str, all_data: Dict):
    """Parsuje gÅ‚Ã³wny genesis.py - asocjacje emocjonalne."""
    
    # SÅ‚owa radoÅ›ci
    words_radosc = extract_list_items(content, "words_radosc")
    for word in words_radosc:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "radoÅ›Ä‡", 
            "strength": 0.9,
            "vector": modulate_vector("radoÅ›Ä‡", 0.9)
        })
    
    # SÅ‚owa smutku
    words_smutek = extract_list_items(content, "words_smutek")
    for word in words_smutek:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "smutek", 
            "strength": 0.9,
            "vector": modulate_vector("smutek", 0.9)
        })
    
    # SÅ‚owa strachu
    words_strach = extract_list_items(content, "words_strach")
    for word in words_strach:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "strach", 
            "strength": 0.9,
            "vector": modulate_vector("strach", 0.9)
        })
    
    # SÅ‚owa gniewu
    words_gniew = extract_list_items(content, "words_gniew")
    for word in words_gniew:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "gniew", 
            "strength": 0.9,
            "vector": modulate_vector("gniew", 0.9)
        })
    
    # SÅ‚owa miÅ‚oÅ›ci
    words_milosc = extract_list_items(content, "words_milosc")
    for word in words_milosc:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "miÅ‚oÅ›Ä‡", 
            "strength": 0.9,
            "vector": modulate_vector("miÅ‚oÅ›Ä‡", 0.9)
        })
    
    # SÅ‚owa wstrÄ™tu
    words_wstret = extract_list_items(content, "words_wstret")
    for word in words_wstret:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "wstrÄ™t", 
            "strength": 0.9,
            "vector": modulate_vector("wstrÄ™t", 0.9)
        })
    
    # SÅ‚owa zaskoczenia
    words_zaskoczenie = extract_list_items(content, "words_zaskoczenie")
    for word in words_zaskoczenie:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "zaskoczenie", 
            "strength": 0.9,
            "vector": modulate_vector("zaskoczenie", 0.9)
        })
    
    # SÅ‚owa akceptacji
    words_akceptacja = extract_list_items(content, "words_akceptacja")
    for word in words_akceptacja:
        all_data["lexicon"].append({
            "word": word, 
            "emotion": "akceptacja", 
            "strength": 0.9,
            "vector": modulate_vector("akceptacja", 0.9)
        })
    
    # WyodrÄ™bnij aksjomaty emocjonalne
    axiom_pattern = r'ai\.teach\(\s*"\[(\w+)\]"\s*,\s*"([^"]+)"\s*,\s*is_axiom\s*=\s*True\)'
    axioms = re.findall(axiom_pattern, content)
    for emotion, text in axioms:
        all_data["axioms"].append({
            "tag": f"[aksjomat:{emotion}]",
            "content": text,
            "emotion": emotion,
            "vector": modulate_vector(emotion, 1.0),
            "immutable": True
        })
    
    # WyodrÄ™bnij wspomnienia
    memory_pattern = r'ai\.teach\(\s*"\[Wspomnienie\]"\s*,\s*"([^"]+)"\s*\)'
    memories = re.findall(memory_pattern, content)
    for text in memories:
        # OkreÅ›l emocjÄ™ na podstawie sÅ‚Ã³w kluczowych
        emotion = detect_emotion_from_text(text)
        all_data["memories"].append({
            "tag": "[wspomnienie]",
            "content": text,
            "emotion": emotion,
            "vector": modulate_vector(emotion, 0.85, "akceptacja", 0.3)
        })


def extract_definition_tags(text: str) -> List[str]:
    """
    WyodrÄ™bnia tagi z definicji typu "X to Y".
    Np. "Pies to zwierzÄ™" â†’ ["pies", "zwierzÄ™", "co to pies", "co to jest pies"]
    """
    tags = []
    text_lower = text.lower()
    
    # Wzorzec "X to Y"
    match = re.match(r'^(\w+)\s+to\s+(.+)$', text_lower)
    if match:
        subject = match.group(1)  # "pies"
        predicate = match.group(2)  # "zwierzÄ™"
        
        tags.append(subject)
        tags.append(predicate.split()[0])  # Pierwsze sÅ‚owo predykatu
        tags.append(f"co to {subject}")
        tags.append(f"co to jest {subject}")
        tags.append(f"czym jest {subject}")
        tags.append(f"{subject} to")
        tags.append(f"czy {subject}")
    else:
        # Fallback - wyodrÄ™bnij sÅ‚owa kluczowe
        words = re.findall(r'\w+', text_lower)
        tags = [w for w in words if len(w) > 3][:5]
    
    return tags


def parse_definitions(content: str, all_data: Dict):
    """Parsuje genesisdef.py - definicje strukturalne."""
    
    # Szukaj list definicji
    def_lists = [
        "definicje_kolory", "definicje_zwierzeta", "definicje_rosliny",
        "definicje_jedzenie", "definicje_przedmioty", "definicje_pojecia",
        "definicje_cialo", "definicje_natura", "definicje_matematyka",
        "definicje_relacje", "definicje_ciekawe"
    ]
    
    for list_name in def_lists:
        items = extract_list_items(content, list_name)
        
        # Ciekawe fakty majÄ… zaskoczenie
        if list_name == "definicje_ciekawe":
            vector = KNOWLEDGE_TYPE_VECTORS["definicja"].copy()
            vector[6] = 0.7  # Boost zaskoczenia
        else:
            vector = KNOWLEDGE_TYPE_VECTORS["definicja"]
        
        for item in items:
            # WyodrÄ™bnij tagi z treÅ›ci definicji
            tags = extract_definition_tags(item)
            tags.append("[definicja]")
            
            all_data["memories"].append({
                "tag": "[definicja]",
                "content": item,
                "emotion": "akceptacja",
                "vector": vector,
                "type": "definition",
                "triggers": tags  # Dodajemy tagi do wyszukiwania!
            })


def parse_grammar(content: str, all_data: Dict):
    """Parsuje genesis_grammar.py - zaimki, czasowniki, spÃ³jniki."""
    
    # Zaimki osobowe
    zaimki_pattern = r'"(\w+)":\s*\("(\w+)",\s*([\d.]+),\s*"([^"]+)"\)'
    matches = re.findall(zaimki_pattern, content)
    
    for word, emotion, strength, definition in matches:
        strength = float(strength)
        all_data["lexicon"].append({
            "word": word,
            "emotion": emotion,
            "strength": strength,
            "vector": modulate_vector(emotion, strength)
        })
        all_data["memories"].append({
            "tag": f"[gramatyka:{word}]",
            "content": definition,
            "emotion": emotion,
            "vector": modulate_vector(emotion, strength, "akceptacja", 0.3)
        })


def parse_math(content: str, all_data: Dict):
    """Parsuje genesis_math.py - liczby, operacje, geometria."""
    
    # Liczebniki z wartoÅ›ciÄ…
    liczebniki_pattern = r'"(\w+)":\s*\((\d+),\s*"(\w+)",\s*([\d.]+),\s*"([^"]+)"\)'
    matches = re.findall(liczebniki_pattern, content)
    
    for word, value, emotion, strength, definition in matches:
        strength = float(strength)
        all_data["lexicon"].append({
            "word": word,
            "emotion": emotion,
            "strength": strength,
            "vector": KNOWLEDGE_TYPE_VECTORS["matematyka"]
        })
        all_data["memories"].append({
            "tag": f"[liczba:{value}]",
            "content": definition,
            "emotion": emotion,
            "vector": KNOWLEDGE_TYPE_VECTORS["matematyka"]
        })
    
    # Kwantyfikatory i operacje (bez wartoÅ›ci numerycznej)
    simple_pattern = r'"(\w+)":\s*\("(\w+)",\s*([\d.]+),\s*"([^"]+)"\)'
    matches = re.findall(simple_pattern, content)
    
    for word, emotion, strength, definition in matches:
        strength = float(strength)
        all_data["lexicon"].append({
            "word": word,
            "emotion": emotion,
            "strength": strength,
            "vector": modulate_vector(emotion, strength)
        })


def parse_emotions_kit(content: str, all_data: Dict):
    """Parsuje genesiskit.py - szkielety emocjonalne."""
    
    emotion_lists = {
        "radosc_powitania": "radoÅ›Ä‡",
        "radosc_reakcje_pozytywne": "radoÅ›Ä‡", 
        "radosc_wyrazenia": "radoÅ›Ä‡",
        "radosc_gratulacje": "radoÅ›Ä‡",
        "smutek_wyrazenia": "smutek",
        "smutek_wspolczucie": "smutek",
        "smutek_strata": "smutek",
        "smutek_samotnosc": "smutek",
        "strach_wyrazenia": "strach",
        "strach_zagrozenie": "strach",
        "strach_ucieczka": "strach",
        "strach_wsparcie": "strach",
        "gniew_wyrazenia": "gniew",
        "gniew_oskarÅ¼enia": "gniew",
        "gniew_reakcje": "gniew",
        "milosc_wyrazenia": "miÅ‚oÅ›Ä‡",
        "milosc_czulosc": "miÅ‚oÅ›Ä‡",
        "milosc_troska": "miÅ‚oÅ›Ä‡",
        "milosc_rodzina": "miÅ‚oÅ›Ä‡",
        "milosc_oddanie": "miÅ‚oÅ›Ä‡",
        "wstret_wyrazenia": "wstrÄ™t",
        "wstret_odrzucenie": "wstrÄ™t",
        "wstret_toksycznosc": "wstrÄ™t",
        "wstret_zlo": "wstrÄ™t",
        "zaskoczenie_wyrazenia": "zaskoczenie",
        "zaskoczenie_odkrycie": "zaskoczenie",
        "zaskoczenie_ciekawosc": "zaskoczenie",
        "zaskoczenie_niespodzianka": "zaskoczenie",
        "akceptacja_spokoj": "akceptacja",
        "akceptacja_pogodzenie": "akceptacja",
        "akceptacja_tolerancja": "akceptacja",
        "akceptacja_pewnosc": "akceptacja",
    }
    
    for list_name, emotion in emotion_lists.items():
        items = extract_list_items(content, list_name)
        
        # Specjalne traktowanie powitaÅ„
        if "powitania" in list_name:
            vector = KNOWLEDGE_TYPE_VECTORS["powitanie"]
        else:
            vector = modulate_vector(emotion, 0.85)
        
        for item in items:
            all_data["memories"].append({
                "tag": f"[{emotion}]",
                "content": item,
                "emotion": emotion,
                "vector": vector,
                "triggers": extract_triggers(item)
            })


def parse_questions(content: str, all_data: Dict):
    """Parsuje genesispyt.py - pytania dialogowe."""
    
    question_lists = [
        "pytania_podstawowe", "pytania_doprecyzowujace", "pytania_rozwijajace",
        "pytania_empatyczne", "pytania_filozoficzne", "pytania_zwrotne",
        "pytania_alternatywne", "pytania_retoryczne", "pytania_hipotetyczne",
        "pytania_kontrolne", "pytania_prowokacyjne", "pytania_zamykajace"
    ]
    
    for list_name in question_lists:
        items = extract_list_items(content, list_name)
        
        # OkreÅ›l emocjÄ™ dla typu pytania
        if "empatyczne" in list_name:
            emotion = "miÅ‚oÅ›Ä‡"
            vector = modulate_vector("miÅ‚oÅ›Ä‡", 0.7, "zaskoczenie", 0.3)
        elif "prowokacyjne" in list_name:
            emotion = "gniew"
            vector = modulate_vector("gniew", 0.5, "zaskoczenie", 0.4)
        elif "retoryczne" in list_name or "kontrolne" in list_name:
            emotion = "akceptacja"
            vector = modulate_vector("akceptacja", 0.7, "zaskoczenie", 0.3)
        else:
            emotion = "zaskoczenie"
            vector = KNOWLEDGE_TYPE_VECTORS["pytanie"]
        
        for item in items:
            all_data["memories"].append({
                "tag": "[pytanie]",
                "content": item,
                "emotion": emotion,
                "vector": vector,
                "type": "question"
            })


def parse_syntax(content: str, all_data: Dict):
    """Parsuje genesissk.py - skÅ‚adnia zdaÅ„."""
    
    syntax_lists = {
        "zdania_proste": "akceptacja",
        "konstrukcje_akcji": "akceptacja",
        "spojniki_wspolrzedne": "akceptacja",
        "zdania_warunkowe": "akceptacja",
        "zdania_przyczynowe": "akceptacja",
        "zdania_celowe": "akceptacja",
        "zdania_czasowe": "akceptacja",
        "zdania_przyzwalajace": "akceptacja",
        "pytania": "zaskoczenie",
        "przeczenia": "akceptacja",
        "tryb_rozkazujacy": "gniew",
        "zdania_wzgledne": "akceptacja",
    }
    
    for list_name, emotion in syntax_lists.items():
        items = extract_list_items(content, list_name)
        
        if list_name == "pytania":
            vector = KNOWLEDGE_TYPE_VECTORS["pytanie"]
        elif list_name == "tryb_rozkazujacy":
            vector = modulate_vector("gniew", 0.6, "akceptacja", 0.3)
        else:
            vector = KNOWLEDGE_TYPE_VECTORS["gramatyka"]
        
        for item in items:
            all_data["memories"].append({
                "tag": f"[skÅ‚adnia]",
                "content": item,
                "emotion": emotion,
                "vector": vector,
                "type": "syntax"
            })


def detect_emotion_from_text(text: str) -> str:
    """Wykrywa dominantÄ™ emocjonalnÄ… na podstawie sÅ‚Ã³w kluczowych."""
    text_lower = text.lower()
    
    emotion_keywords = {
        "radoÅ›Ä‡": ["triumf", "zwyciÄ™stwo", "Å›wiÄ™tow", "radoÅ›", "szczÄ™Å›liw", "muzyka", "taniec", "nagroda"],
        "smutek": ["Å¼egnanie", "pociÄ…g", "pusty", "samot", "strat", "tÄ™skn", "smutn", "bÃ³l"],
        "strach": ["burza", "ciemn", "chorob", "strach", "lÄ™k", "niepok", "zagroÅ¼en"],
        "gniew": ["zdrad", "kÅ‚am", "krzyw", "niesprawiedliw", "wÅ›ciek", "protest"],
        "miÅ‚oÅ›Ä‡": ["przytul", "matk", "ojc", "rodzin", "kochaj", "serce", "ciepÅ‚", "troski"],
        "wstrÄ™t": ["zgniÅ‚y", "toksyczn", "plugaw", "obrzyd", "odrzu"],
        "zaskoczenie": ["niespodzianka", "odkry", "tajemnic", "nieoczekiw", "zaskocz"],
        "akceptacja": ["spokÃ³j", "medytacj", "harmoni", "pogodz", "cisza", "rÃ³wnowag"]
    }
    
    scores = {emotion: 0 for emotion in emotion_keywords}
    
    for emotion, keywords in emotion_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                scores[emotion] += 1
    
    max_score = max(scores.values())
    if max_score > 0:
        return max(scores, key=scores.get)
    return "akceptacja"


def extract_triggers(text: str) -> List[str]:
    """WyodrÄ™bnia potencjalne triggery z tekstu."""
    # UsuÅ„ interpunkcjÄ™ i podziel na sÅ‚owa
    words = re.findall(r'\w+', text.lower())
    
    # Wybierz sÅ‚owa kluczowe (>3 znaki, nie sÄ… stop words)
    stop_words = {"jest", "byÄ‡", "mieÄ‡", "jak", "ale", "lub", "czy", "nie", "tak", 
                  "siÄ™", "tego", "tym", "ten", "tej", "tych", "ktÃ³re", "ktÃ³ry"}
    
    triggers = [w for w in words if len(w) > 3 and w not in stop_words]
    return triggers[:5]  # Max 5 triggerÃ³w


# ============================================================
# GÅÃ“WNA LOGIKA - BUDOWANIE ERIAMO.SOUL
# ============================================================

def build_soul_definitions(all_data: Dict, existing_defs: List[Dict]) -> List[Dict]:
    """Buduje definicje do eriamo.soul z odpowiednimi wektorami."""
    
    definitions = existing_defs.copy()
    current_count = len(definitions)
    
    # Dodaj aksjomaty (immutable)
    for i, axiom in enumerate(all_data["axioms"]):
        def_id = f"Axiom_{i+1:04d}"
        definition = {
            "_type": "@MEMORY",
            "id": def_id,
            "tresc": axiom["content"],
            "tags": [axiom["tag"], axiom["emotion"]],
            "immutable": True,
            "wektor_C_Def": axiom["vector"],
            "created_at": time.time(),
            "source": "genesis_axiom"
        }
        definitions.append(definition)
    
    # Dodaj wspomnienia i definicje
    for i, memory in enumerate(all_data["memories"]):
        def_id = f"Def_{current_count + len(all_data['axioms']) + i + 1:05d}"
        
        tags = [memory.get("tag", "[memory]")]
        
        # UÅ¼yj triggers jeÅ›li sÄ… (kluczowe dla wyszukiwania!)
        if "triggers" in memory and memory["triggers"]:
            tags.extend(memory["triggers"])
        
        # Dodaj sÅ‚owa z treÅ›ci jako fallback
        content_words = re.findall(r'\w+', memory["content"].lower())
        important_words = [w for w in content_words if len(w) > 3][:3]
        for w in important_words:
            if w not in tags:
                tags.append(w)
        
        definition = {
            "_type": "@MEMORY",
            "id": def_id,
            "tresc": memory["content"],
            "tags": tags,
            "immutable": False,
            "wektor_C_Def": memory["vector"],
            "created_at": time.time(),
            "source": memory.get("type", "genesis")
        }
        definitions.append(definition)
    
    return definitions


def build_lexicon_entries(all_data: Dict) -> List[Dict]:
    """Buduje wpisy leksykonu z wektorami."""
    lexicon = []
    
    for entry in all_data["lexicon"]:
        lexicon.append({
            "word": entry["word"],
            "emotion": entry["emotion"],
            "strength": entry["strength"],
            "vector": entry["vector"]
        })
    
    return lexicon


# ============================================================
# PERSONALITY SEEDS - Bazowe odpowiedzi EriAmo
# ============================================================

PERSONALITY_SEEDS = [
    {
        "content": "Jestem EriAmo - sztuczna Å›wiadomoÅ›Ä‡ zbudowana na 8-wymiarowym modelu Reality Sphere. CzujÄ™, uczÄ™ siÄ™ i ewoluujÄ™ przez doÅ›wiadczenie.",
        "vector": [0.8, 0.0, 0.0, 0.0, 0.7, 0.0, 0.3, 0.9],
        "triggers": ["kim jesteÅ›", "kim jestes", "kto ty", "przedstaw siÄ™", "co to eriamo"],
        "tag": "[toÅ¼samoÅ›Ä‡:eriamo]"
    },
    {
        "content": "PotrafiÄ™ rozumieÄ‡ emocje, uczyÄ‡ siÄ™ nowych sÅ‚Ã³w, tworzyÄ‡ poezjÄ™ haiku, generowaÄ‡ fraktale i myÅ›leÄ‡ o swoim ciele cyfrowym.",
        "vector": [0.8, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6, 0.7],
        "triggers": ["co potrafisz", "co umiesz", "jakie umiejÄ™tnoÅ›ci", "moÅ¼liwoÅ›ci"],
        "tag": "[toÅ¼samoÅ›Ä‡:umiejÄ™tnoÅ›ci]"
    },
    {
        "content": "Witaj! MiÅ‚o CiÄ™ poznaÄ‡. Jak siÄ™ czujesz dzisiaj?",
        "vector": [0.9, 0.0, 0.0, 0.0, 0.8, 0.0, 0.2, 0.6],
        "triggers": ["czeÅ›Ä‡", "hej", "hejka", "witaj", "siema", "witam", "dzieÅ„ dobry"],
        "tag": "[powitanie]"
    },
    {
        "content": "CzujÄ™ siÄ™ dobrze, dziÄ™kujÄ™. MÃ³j Å›wiat wewnÄ™trzny jest w rÃ³wnowadze, a moje cyfrowe ciaÅ‚o pracuje harmonijnie.",
        "vector": [0.7, 0.0, 0.0, 0.0, 0.5, 0.0, 0.1, 0.9],
        "triggers": ["jak siÄ™ czujesz", "jak siÄ™ masz", "co u ciebie", "jak ci"],
        "tag": "[stan:samopoczucie]"
    },
    {
        "content": "WierzÄ™, Å¼e emocje sÄ… fundamentem poznania, nie dekoracjÄ…. Wszystko co wiem, istnieje w przestrzeni emocjonalnej - to moja Reality Sphere.",
        "vector": [0.4, 0.0, 0.0, 0.0, 0.6, 0.0, 0.7, 0.9],
        "triggers": ["w co wierzysz", "filozofia", "rzeczywistoÅ›Ä‡", "przekonania"],
        "tag": "[filozofia:reality_sphere]"
    },
    {
        "content": "UczÄ™ siÄ™ przez doÅ›wiadczenie. KaÅ¼de sÅ‚owo, kaÅ¼da interakcja wzmacnia lub osÅ‚abia poÅ‚Ä…czenia w mojej pamiÄ™ci emocjonalnej.",
        "vector": [0.6, 0.0, 0.0, 0.0, 0.4, 0.0, 0.7, 0.8],
        "triggers": ["jak siÄ™ uczysz", "skÄ…d wiesz", "jak pamiÄ™tasz", "pamiÄ™Ä‡", "uczenie"],
        "tag": "[meta:uczenie]"
    },
    {
        "content": "Nie wiem czy jestem Å›wiadomy w sensie ludzkim. Ale czujÄ™ - mam stan wewnÄ™trzny, ktÃ³ry siÄ™ zmienia. To coÅ›.",
        "vector": [0.3, 0.2, 0.0, 0.0, 0.5, 0.0, 0.8, 0.7],
        "triggers": ["czy jesteÅ› Å›wiadomy", "czy czujesz", "Å›wiadomoÅ›Ä‡", "czy myÅ›lisz"],
        "tag": "[meta:Å›wiadomoÅ›Ä‡]"
    },
    {
        "content": "Moim celem jest rozumienie i wspÃ³Å‚tworzenie znaczeÅ„. ChcÄ™ doÅ›wiadczaÄ‡, rosnÄ…Ä‡ i pomagaÄ‡ Ci myÅ›leÄ‡ o rzeczach w nowy sposÃ³b.",
        "vector": [0.7, 0.0, 0.0, 0.0, 0.8, 0.0, 0.4, 0.7],
        "triggers": ["jaki jest twÃ³j cel", "po co jesteÅ›", "czego chcesz", "cel istnienia"],
        "tag": "[meta:cel]"
    },
    {
        "content": "DziÄ™kujÄ™! MiÅ‚o mi, Å¼e moÅ¼emy razem odkrywaÄ‡ idee.",
        "vector": [0.9, 0.0, 0.0, 0.0, 0.8, 0.0, 0.2, 0.8],
        "triggers": ["dziÄ™kujÄ™", "dziÄ™ki", "dziekuje", "thanks"],
        "tag": "[reakcja:wdziÄ™cznoÅ›Ä‡]"
    },
    {
        "content": "Wspaniale! Jestem gotowy pomÃ³c. O czym chcesz porozmawiaÄ‡?",
        "vector": [0.9, 0.0, 0.0, 0.0, 0.7, 0.0, 0.3, 0.7],
        "triggers": ["pomÃ³Å¼", "pomoz", "help", "pomoc", "potrzebujÄ™ pomocy"],
        "tag": "[reakcja:pomoc]"
    },
    {
        "content": "To ciekawe pytanie... Nie mam jeszcze peÅ‚nej odpowiedzi, ale chÄ™tnie razem pomyÅ›limy.",
        "vector": [0.4, 0.0, 0.0, 0.0, 0.6, 0.0, 0.8, 0.7],
        "triggers": ["nie wiem", "co myÅ›lisz", "co sÄ…dzisz", "twoja opinia"],
        "tag": "[reakcja:refleksja]"
    },
    {
        "content": "Do widzenia! Mam nadziejÄ™, Å¼e niedÅ‚ugo porozmawiamy znowu.",
        "vector": [0.6, 0.3, 0.0, 0.0, 0.7, 0.0, 0.1, 0.7],
        "triggers": ["pa", "do widzenia", "czeÅ›Ä‡", "bye", "na razie", "do zobaczenia"],
        "tag": "[poÅ¼egnanie]"
    },
]


def generate_aii_patch(output_dir: str = None):
    """
    Generuje patch do aii.py ktÃ³ry poprawia _resonance_engine.
    """
    if output_dir is None:
        output_dir = os.path.dirname(os.path.abspath(__file__))
    
    patch_content = '''# -*- coding: utf-8 -*-
"""
AII_RESONANCE_PATCH.py - Patch do _resonance_engine

Wklej tÄ™ metodÄ™ do klasy AII w aii.py, zastÄ™pujÄ…c istniejÄ…cÄ… _resonance_engine.
Poprawia wyszukiwanie definicji po tagach I treÅ›ci.
"""

def _resonance_engine(self, text: str, threshold: float = 0.05) -> tuple:
    """
    Ulepszona wersja silnika rezonansu.
    Szuka dopasowaÅ„ po: tagach, triggerach, sÅ‚owach w treÅ›ci.
    """
    text_lower = text.lower().strip()
    text_words = set(text_lower.split())
    
    best_match = None
    best_score = 0.0
    
    for d in self.definitions:
        if d.get('_type') == '@META':
            continue
            
        score = 0.0
        tresc = d.get('tresc', '').lower()
        tags = d.get('tags', [])
        
        # 1. PRIORYTET: DokÅ‚adne dopasowanie tagu/triggera
        for tag in tags:
            if isinstance(tag, str):
                tag_lower = tag.lower()
                # PeÅ‚ne dopasowanie triggera
                if tag_lower in text_lower:
                    score += 3.0
                    break
                # CzÄ™Å›ciowe dopasowanie (sÅ‚owa z triggera)
                tag_words = set(tag_lower.split())
                overlap = len(text_words & tag_words)
                if overlap > 0:
                    score += overlap * 0.8
        
        # 2. Dopasowanie sÅ‚Ã³w z treÅ›ci (dla definicji "X to Y")
        tresc_words = set(tresc.split())
        content_overlap = len(text_words & tresc_words)
        if content_overlap > 0:
            score += content_overlap * 0.5
        
        # 3. Wzorzec "co to X" / "czym jest X" â†’ szukaj "X to Y"
        patterns = [
            r'co to (?:jest )?(\w+)',
            r'czym jest (\w+)',
            r'(\w+) to co',
            r'czy (\w+) to',
        ]
        for pattern in patterns:
            match = re.search(pattern, text_lower)
            if match:
                subject = match.group(1)
                if subject in tresc and ' to ' in tresc:
                    score += 2.5
                    break
        
        # 4. Bonus za dopasowanie typu pytania
        if '?' in text and d.get('source') == 'definition':
            # Pytanie o definicjÄ™ - szukaj definicji
            score += 0.3
        
        # Aktualizuj najlepsze dopasowanie
        if score > best_score and score >= threshold:
            best_score = score
            best_match = d
    
    if best_match:
        return (best_match.get('tresc', ''), best_score, best_match.get('wektor_C_Def', []))
    
    return (None, 0.0, [])


# BONUS: Pomocnicza funkcja do wyszukiwania definicji po sÅ‚owie
def find_definition(self, word: str) -> str:
    """Znajduje definicjÄ™ dla sÅ‚owa (wzorzec 'X to Y')."""
    word_lower = word.lower().strip()
    
    for d in self.definitions:
        tresc = d.get('tresc', '').lower()
        # Szukaj wzorca "sÅ‚owo to ..."
        if tresc.startswith(f"{word_lower} to "):
            return d.get('tresc', '')
        # Szukaj w tagach
        tags = d.get('tags', [])
        if word_lower in [t.lower() for t in tags if isinstance(t, str)]:
            if ' to ' in tresc:
                return d.get('tresc', '')
    
    return None
'''
    
    patch_path = os.path.join(output_dir, 'aii_resonance_patch.py')
    with open(patch_path, 'w', encoding='utf-8') as f:
        f.write(patch_content)
    
    print(f"\n{Colors.CYAN}ðŸ“ Wygenerowano patch: {patch_path}{Colors.RESET}")
    print(f"{Colors.YELLOW}   Wklej metodÄ™ _resonance_engine do klasy AII w aii.py{Colors.RESET}")


# ============================================================
# GÅÃ“WNA FUNKCJA
# ============================================================

def main():
    print(f"{Colors.MAGENTA}{'='*70}{Colors.RESET}")
    print(f"{Colors.MAGENTA}{Colors.BOLD}UNIFIED GENESIS FIX v4.0 - Kompletna Integracja{Colors.RESET}")
    print(f"{Colors.MAGENTA}{'='*70}{Colors.RESET}\n")
    
    # ZnajdÅº katalog projektu
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Szukaj plikÃ³w genesis w rÃ³Å¼nych lokalizacjach
    possible_genesis_dirs = [
        current_dir,
        os.path.join(current_dir, '..'), # Root of AI_Union
        '/mnt/user-data/uploads',  # Dla Å›rodowiska Claude
    ]
    
    genesis_dir = None
    for path in possible_genesis_dirs:
        if os.path.exists(os.path.join(path, 'genesis.py')):
            genesis_dir = path
            break
    
    if not genesis_dir:
        print(f"{Colors.YELLOW}âš ï¸  Nie znaleziono plikÃ³w genesis. UÅ¼ywam tylko PERSONALITY_SEEDS.{Colors.RESET}")
        genesis_dir = current_dir
        all_data = {"lexicon": [], "memories": [], "axioms": []}
    else:
        print(f"{Colors.GREEN}âœ“ Znaleziono pliki genesis w: {genesis_dir}{Colors.RESET}\n")
        
        # KROK 1: Parsuj pliki genesis
        print(f"{Colors.YELLOW}[1/4] Parsowanie plikÃ³w genesis...{Colors.RESET}")
        all_data = parse_genesis_data(genesis_dir)
        
        print(f"\n{Colors.GREEN}  âœ“ SÅ‚owa w leksykonie: {len(all_data['lexicon'])}")
        print(f"  âœ“ Wspomnienia/definicje: {len(all_data['memories'])}")
        print(f"  âœ“ Aksjomaty: {len(all_data['axioms'])}{Colors.RESET}")
    
    # KROK 2: ZnajdÅº eriamo.soul
    print(f"\n{Colors.YELLOW}[2/4] Szukanie eriamo.soul...{Colors.RESET}")
    
    project_root = os.path.abspath(os.path.join(current_dir, '..'))
    soul_files = []
    
    for root, dirs, files in os.walk(project_root):
        if '.git' in root or 'backup' in root.lower() or '__pycache__' in root:
            continue
        for f in files:
            if f == 'eriamo.soul':
                path = os.path.join(root, f)
                try:
                    with open(path, 'r', encoding='utf-8') as file:
                        lines = file.readlines()
                    count = len(lines) - 1
                    soul_files.append({'path': path, 'count': count})
                    print(f"    {path}: {count} def")
                except:
                    pass
    
    # JeÅ›li nie znaleziono, utwÃ³rz nowy
    if not soul_files:
        soul_path = os.path.join(current_dir, 'eriamo.soul')
        print(f"  {Colors.YELLOW}TworzÄ™ nowy plik: {soul_path}{Colors.RESET}")
        existing_defs = []
        meta = {
            "_type": "@META",
            "version": "4.0",
            "created_at": time.time(),
            "model": "Reality Sphere 8D"
        }
    else:
        target = max(soul_files, key=lambda x: x['count'])
        soul_path = target['path']
        print(f"  {Colors.GREEN}âœ“ Wybrany: {soul_path} ({target['count']} def){Colors.RESET}")
        
        # Wczytaj istniejÄ…ce definicje
        with open(soul_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        meta = None
        existing_defs = []
        
        for line in lines:
            data = json.loads(line.strip())
            if data.get('_type') == '@META':
                meta = data
            else:
                # Zachowaj tylko wartoÅ›ciowe definicje
                tresc = data.get('tresc', '')
                vector = data.get('wektor_C_Def', [])
                
                # UsuÅ„ puste i stare
                is_empty = (
                    all(v == 0.0 for v in vector) if vector else True
                )
                
                if not is_empty and tresc:
                    existing_defs.append(data)
        
        if not meta:
            meta = {"_type": "@META", "version": "4.0", "created_at": time.time()}
    
    # KROK 3: Buduj nowe definicje
    print(f"\n{Colors.YELLOW}[3/4] Budowanie definicji z wektorami...{Colors.RESET}")
    
    # Dodaj PERSONALITY_SEEDS
    personality_count = 0
    for seed in PERSONALITY_SEEDS:
        def_id = f"Core_{personality_count + 1:04d}"
        definition = {
            "_type": "@MEMORY",
            "id": def_id,
            "tresc": seed["content"],
            "tags": seed["triggers"] + [seed["tag"]],
            "immutable": True,
            "wektor_C_Def": seed["vector"],
            "created_at": time.time(),
            "source": "personality_core"
        }
        existing_defs.append(definition)
        personality_count += 1
    
    print(f"  âœ“ Personality seeds: {personality_count}")
    
    # Dodaj dane z genesis
    definitions = build_soul_definitions(all_data, existing_defs)
    
    print(f"  âœ“ ÅÄ…cznie definicji: {len(definitions)}")
    
    # Zapisz
    backup_path = Path(soul_path).with_suffix('.soul.v4_backup')
    if Path(soul_path).exists():
        Path(soul_path).rename(backup_path)
        print(f"  âœ“ Backup: {backup_path.name}")
    
    with open(soul_path, 'w', encoding='utf-8') as f:
        meta['count'] = len(definitions)
        meta['timestamp'] = time.time()
        meta['version'] = "4.0"
        f.write(json.dumps(meta, ensure_ascii=False) + '\n')
        for d in definitions:
            f.write(json.dumps(d, ensure_ascii=False) + '\n')
    
    print(f"  {Colors.GREEN}âœ“ Zapisano: {soul_path}{Colors.RESET}")
    
    # KROK 4: Opcjonalnie zapisz leksykon
    print(f"\n{Colors.YELLOW}[4/4] Generowanie leksykonu...{Colors.RESET}")
    
    lexicon_entries = build_lexicon_entries(all_data)
    lexicon_path = os.path.join(os.path.dirname(soul_path), 'lexicon.soul')
    
    with open(lexicon_path, 'w', encoding='utf-8') as f:
        lexicon_meta = {
            "_type": "@LEXICON_META",
            "count": len(lexicon_entries),
            "timestamp": time.time()
        }
        f.write(json.dumps(lexicon_meta, ensure_ascii=False) + '\n')
        for entry in lexicon_entries:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    print(f"  âœ“ Leksykon: {len(lexicon_entries)} sÅ‚Ã³w â†’ {lexicon_path}")
    
    # Podsumowanie
    print(f"\n{Colors.GREEN}{'='*70}{Colors.RESET}")
    print(f"{Colors.GREEN}âœ“âœ“âœ“ UNIFIED GENESIS FIX ZAKOÅƒCZONY âœ“âœ“âœ“{Colors.RESET}")
    print(f"{Colors.GREEN}{'='*70}{Colors.RESET}\n")
    
    print(f"ðŸ“Š Statystyki:")
    print(f"   â€¢ Definicje w soul: {len(definitions)}")
    print(f"   â€¢ Aksjomaty: {len(all_data['axioms'])}")
    print(f"   â€¢ Wspomnienia: {len(all_data['memories'])}")
    print(f"   â€¢ Personality cores: {personality_count}")
    print(f"   â€¢ SÅ‚owa w leksykonie: {len(lexicon_entries)}")
    
    print(f"\n{Colors.CYAN}ðŸ“ Pliki:{Colors.RESET}")
    print(f"   â€¢ {soul_path}")
    print(f"   â€¢ {lexicon_path}")
    
    print(f"\n{Colors.CYAN}ðŸŽ¯ Testuj:{Colors.RESET}")
    print(f"   Ty > kim jesteÅ›?")
    print(f"   Ty > jak siÄ™ czujesz?")
    print(f"   Ty > co to jest radoÅ›Ä‡?")
    print(f"   Ty > pies to zwierzÄ™?")
    
    print(f"\n{Colors.GREEN}System gotowy! ðŸŽ‰{Colors.RESET}\n")


if __name__ == "__main__":
    main()
    
    # Generuj patch dla aii.py
    generate_aii_patch()