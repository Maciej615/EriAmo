# -*- coding: utf-8 -*-
"""
chunk_lexicon.py v1.0.0 - Chunk-Based Language Processing
EriAmo Union - Nowa architektura jÄ™zykowa bazujÄ…ca na sekwencjach

INSPIRACJA: Nature Human Behaviour (2026)
"JÄ™zyk to nie hierarchiczne drzewo skÅ‚adniowe, ale biblioteka gotowych sekwencji"

KLUCZOWE ZMIANY:
1. Zamiast pojedynczych sÅ‚Ã³w â†’ CHUNKS (sekwencje 2-5 sÅ‚Ã³w)
2. Zamiast reguÅ‚ gramatycznych â†’ WZORCE STATYSTYCZNE
3. Zamiast parsing â†’ PATTERN MATCHING
4. Dodano PRIMING (przyÅ›pieszenie po pierwszym kontakcie)

Autor: Claude & Maciej (bazujÄ…c na badaniach Christiansen et al.)
Data: 2025-01-25
"""

import numpy as np
import json
import time
import re
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter
from union_config import UnionConfig


# =============================================================================
# CHUNK DEFINITION
# =============================================================================

class LanguageChunk:
    """
    Chunk jÄ™zykowy - zapamiÄ™tana sekwencja sÅ‚Ã³w.
    
    PRZYKÅADY:
    - "can I have a" (niekonstytutywa - nie jest frazÄ…!)
    - "in the middle of the"
    - "czy mogÄ™ prosiÄ‡ o"
    - "w Å›rodku"
    
    Chunks sÄ… ZAPAMIÄ˜TYWANE jako caÅ‚oÅ›Ä‡, nie generowane przez reguÅ‚y.
    """
    
    def __init__(
        self,
        text: str,
        frequency: int = 1,
        emotional_vector: Optional[np.ndarray] = None
    ):
        """
        Args:
            text: Tekst chunka (np. "czy mogÄ™ prosiÄ‡ o")
            frequency: CzÄ™stoÅ›Ä‡ wystÄ™powania
            emotional_vector: Wektor emocjonalny [15D]
        """
        self.text = text.lower().strip()
        self.words = self.text.split()
        self.length = len(self.words)
        self.frequency = frequency
        
        # Wektor emocjonalny (15D)
        if emotional_vector is None:
            self.emotional_vector = np.zeros(UnionConfig.DIMENSION)
        else:
            self.emotional_vector = emotional_vector
        
        # Priming (przyÅ›pieszenie po pierwszym kontakcie)
        self.last_seen = 0.0
        self.priming_strength = 0.0
        
        # Konteksty uÅ¼ycia
        self.contexts = []  # Lista przykÅ‚adowych zdaÅ„ zawierajÄ…cych chunk
    
    def update_priming(self):
        """
        Aktualizuje siÅ‚Ä™ primingu po ponownym napotkaniu.
        
        MECHANIZM Z ARTYKUÅU:
        Po jednokrotnym zetkniÄ™ciu mÃ³zg przetwarza sekwencjÄ™ szybciej.
        """
        current_time = time.time()
        
        # Zanikanie primingu w czasie (half-life = 60 sekund)
        time_diff = current_time - self.last_seen
        decay = np.exp(-time_diff / 60.0)
        
        # Wzrost primingu (im czÄ™Å›ciej widzimy, tym silniejszy)
        self.priming_strength = min(1.0, self.priming_strength * decay + 0.3)
        self.last_seen = current_time
    
    def get_processing_speed_boost(self) -> float:
        """
        Zwraca boost szybkoÅ›ci przetwarzania (0.0 - 1.0).
        
        Returns:
            float: Multiplikator szybkoÅ›ci (1.0 = brak boostu, 2.0 = 2x szybciej)
        """
        return 1.0 + self.priming_strength
    
    def add_context(self, sentence: str):
        """Dodaje przykÅ‚ad uÅ¼ycia chunka."""
        if len(self.contexts) < 10:  # Limit 10 przykÅ‚adÃ³w
            self.contexts.append(sentence)
    
    def to_dict(self) -> dict:
        """Serializacja do JSON."""
        return {
            'text': self.text,
            'frequency': self.frequency,
            'length': self.length,
            'emotional_vector': self.emotional_vector.tolist(),
            'priming_strength': self.priming_strength,
            'contexts': self.contexts[:3]  # Tylko 3 przykÅ‚ady
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'LanguageChunk':
        """Deserializacja z JSON."""
        chunk = cls(
            text=data['text'],
            frequency=data.get('frequency', 1),
            emotional_vector=np.array(data.get('emotional_vector', []))
        )
        chunk.priming_strength = data.get('priming_strength', 0.0)
        chunk.contexts = data.get('contexts', [])
        return chunk


# =============================================================================
# CHUNK-BASED LEXICON
# =============================================================================

class ChunkLexicon:
    """
    Leksykon oparty na sekwencjach (chunks) zamiast pojedynczych sÅ‚Ã³w.
    
    NOWOÅšÄ†:
    - ZapamiÄ™tuje CZÄ˜STE SEKWENCJE (2-5 sÅ‚Ã³w)
    - Automatyczne wykrywanie wzorcÃ³w
    - Priming (przyÅ›pieszenie po pierwszym kontakcie)
    - Statystyczna analiza bez reguÅ‚ gramatycznych
    """
    
    def __init__(self, chunk_file: str = "data/chunks.json"):
        """
        Args:
            chunk_file: ÅšcieÅ¼ka do pliku z chunkami
        """
        self.chunk_file = chunk_file
        
        # Chunks (indeksowane po tekÅ›cie)
        self.chunks: Dict[str, LanguageChunk] = {}
        
        # Statystyki
        self.total_chunks = 0
        self.total_exposures = 0
        
        # Wczytaj z dysku
        self.load()
    
    # =========================================================================
    # CHUNK EXTRACTION (Automatyczne wykrywanie wzorcÃ³w)
    # =========================================================================
    
    def extract_chunks_from_text(self, text: str, min_length: int = 2, max_length: int = 5):
        """
        Ekstraktuje wszystkie moÅ¼liwe chunki z tekstu.
        
        MECHANIZM:
        - Okno przesuwne 2-5 sÅ‚Ã³w
        - Zliczanie czÄ™stoÅ›ci
        - Filtrowanie po czÄ™stoÅ›ci
        
        Args:
            text: Tekst do analizy
            min_length: Min liczba sÅ‚Ã³w w chunku
            max_length: Max liczba sÅ‚Ã³w w chunku
        """
        # Preprocessing
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)  # UsuÅ„ interpunkcjÄ™
        words = text.split()
        
        # Ekstraktuj wszystkie n-gramy
        chunk_candidates = []
        
        for n in range(min_length, max_length + 1):
            for i in range(len(words) - n + 1):
                chunk_text = ' '.join(words[i:i+n])
                chunk_candidates.append(chunk_text)
        
        # Zlicz czÄ™stoÅ›ci
        chunk_counts = Counter(chunk_candidates)
        
        # Dodaj do leksykonu (tylko czÄ™ste)
        for chunk_text, count in chunk_counts.items():
            if count >= 2:  # Minimum 2 wystÄ…pienia
                self.add_or_update_chunk(chunk_text, count)
    
    def add_or_update_chunk(self, chunk_text: str, frequency_boost: int = 1):
        """
        Dodaje chunk lub zwiÄ™ksza jego czÄ™stoÅ›Ä‡.
        
        Args:
            chunk_text: Tekst chunka
            frequency_boost: O ile zwiÄ™kszyÄ‡ czÄ™stoÅ›Ä‡
        """
        chunk_text = chunk_text.lower().strip()
        
        if chunk_text in self.chunks:
            # Aktualizuj istniejÄ…cy
            self.chunks[chunk_text].frequency += frequency_boost
            self.chunks[chunk_text].update_priming()
        else:
            # StwÃ³rz nowy
            self.chunks[chunk_text] = LanguageChunk(chunk_text, frequency_boost)
            self.total_chunks += 1
        
        self.total_exposures += frequency_boost
    
    # =========================================================================
    # PATTERN MATCHING (Rozpoznawanie w tekÅ›cie)
    # =========================================================================
    
    def find_chunks_in_text(self, text: str) -> List[Tuple[str, LanguageChunk, int]]:
        """
        Znajduje wszystkie chunki w tekÅ›cie.
        
        Returns:
            List[(chunk_text, LanguageChunk, position)]
        """
        text_lower = text.lower()
        words = text_lower.split()
        found_chunks = []
        
        # Sortuj chunki po dÅ‚ugoÅ›ci (najdÅ‚uÅ¼sze pierwsze - zachÅ‚anny matching)
        sorted_chunks = sorted(
            self.chunks.items(),
            key=lambda x: x[1].length,
            reverse=True
        )
        
        for chunk_text, chunk in sorted_chunks:
            # ZnajdÅº wszystkie wystÄ…pienia
            chunk_words = chunk.words
            chunk_len = len(chunk_words)
            
            for i in range(len(words) - chunk_len + 1):
                if words[i:i+chunk_len] == chunk_words:
                    found_chunks.append((chunk_text, chunk, i))
                    
                    # Aktualizuj priming
                    chunk.update_priming()
        
        return found_chunks
    
    def analyze_text_chunks(self, text: str) -> Dict[str, any]:
        """
        PeÅ‚na analiza tekstu bazujÄ…ca na chunkach.
        
        Returns:
            dict: {
                'chunks_found': List[str],
                'coverage': float,  # Procent tekstu pokryty chunkami
                'emotional_vector': np.ndarray,
                'priming_boost': float
            }
        """
        found_chunks = self.find_chunks_in_text(text)
        words = text.lower().split()
        
        # Oblicz pokrycie
        covered_words = set()
        for chunk_text, chunk, position in found_chunks:
            for i in range(position, position + chunk.length):
                covered_words.add(i)
        
        coverage = len(covered_words) / len(words) if words else 0.0
        
        # Agreguj wektor emocjonalny (waÅ¼ony czÄ™stoÅ›ciÄ…)
        emotional_vector = np.zeros(UnionConfig.DIMENSION)
        total_weight = 0.0
        
        for chunk_text, chunk, position in found_chunks:
            weight = chunk.frequency * chunk.get_processing_speed_boost()
            emotional_vector += chunk.emotional_vector * weight
            total_weight += weight
        
        if total_weight > 0:
            emotional_vector /= total_weight
        
        # Åšredni boost primingu
        avg_priming = np.mean([
            chunk.get_processing_speed_boost() 
            for _, chunk, _ in found_chunks
        ]) if found_chunks else 1.0
        
        return {
            'chunks_found': [chunk_text for chunk_text, _, _ in found_chunks],
            'coverage': coverage,
            'emotional_vector': emotional_vector,
            'priming_boost': avg_priming,
            'chunk_count': len(found_chunks)
        }
    
    # =========================================================================
    # EMOTIONAL LEARNING (Uczenie emocji dla chunkÃ³w)
    # =========================================================================
    
    def teach_chunk_emotion(
        self,
        chunk_text: str,
        emotional_vector: np.ndarray,
        strength: float = 0.5
    ):
        """
        Uczy chunk konkretnego wektora emocjonalnego.
        
        Args:
            chunk_text: Tekst chunka
            emotional_vector: Wektor emocji [15D]
            strength: SiÅ‚a uczenia (0.0 - 1.0)
        """
        chunk_text = chunk_text.lower().strip()
        
        if chunk_text not in self.chunks:
            self.chunks[chunk_text] = LanguageChunk(chunk_text)
        
        chunk = self.chunks[chunk_text]
        
        # Uczenie z momentum
        chunk.emotional_vector = (
            chunk.emotional_vector * (1.0 - strength) +
            emotional_vector * strength
        )
    
    # =========================================================================
    # PERSISTENCE
    # =========================================================================
    
    def save(self):
        """Zapisuje leksykon do pliku."""
        import os
        os.makedirs(os.path.dirname(self.chunk_file), exist_ok=True)
        
        data = {
            'version': '1.0.0',
            'total_chunks': self.total_chunks,
            'total_exposures': self.total_exposures,
            'chunks': {
                text: chunk.to_dict()
                for text, chunk in self.chunks.items()
            }
        }
        
        with open(self.chunk_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self):
        """Wczytuje leksykon z pliku."""
        import os
        if not os.path.exists(self.chunk_file):
            return
        
        try:
            with open(self.chunk_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.total_chunks = data.get('total_chunks', 0)
            self.total_exposures = data.get('total_exposures', 0)
            
            for text, chunk_data in data.get('chunks', {}).items():
                self.chunks[text] = LanguageChunk.from_dict(chunk_data)
        
        except Exception as e:
            print(f"[ChunkLexicon] BÅ‚Ä…d wczytywania: {e}")
    
    # =========================================================================
    # STATISTICS
    # =========================================================================
    
    def get_statistics(self) -> dict:
        """Zwraca statystyki leksykonu."""
        if not self.chunks:
            return {'total_chunks': 0}
        
        frequencies = [c.frequency for c in self.chunks.values()]
        lengths = [c.length for c in self.chunks.values()]
        
        return {
            'total_chunks': len(self.chunks),
            'total_exposures': self.total_exposures,
            'avg_frequency': np.mean(frequencies),
            'max_frequency': max(frequencies),
            'avg_length': np.mean(lengths),
            'most_common': self.get_most_common_chunks(5)
        }
    
    def get_most_common_chunks(self, n: int = 10) -> List[Tuple[str, int]]:
        """Zwraca n najczÄ™stszych chunkÃ³w."""
        sorted_chunks = sorted(
            self.chunks.items(),
            key=lambda x: x[1].frequency,
            reverse=True
        )
        return [(text, chunk.frequency) for text, chunk in sorted_chunks[:n]]


# =============================================================================
# INTEGRATION WITH AII
# =============================================================================

def upgrade_aii_to_chunks():
    """
    PrzykÅ‚ad integracji z aii.py.
    
    ZAMIAST:
    - Pojedyncze sÅ‚owa
    - ReguÅ‚y gramatyczne
    
    TERAZ:
    - Chunki (sekwencje)
    - Statystyczne wzorce
    """
    example = '''
# PRZED (aii.py - stara metoda):
class EvolvingLexicon:
    def analyze_text(self, text):
        words = text.split()
        for word in words:
            # Analiza POJEDYNCZYCH SÅÃ“W
            self.learn_word(word)

# PO (chunk_lexicon.py - nowa metoda):
class ChunkLexicon:
    def analyze_text_chunks(self, text):
        # 1. Wykryj chunki automatycznie
        self.extract_chunks_from_text(text)
        
        # 2. ZnajdÅº znane chunki
        found = self.find_chunks_in_text(text)
        
        # 3. Oblicz wektor emocjonalny z CHUNKÃ“W
        result = self.analyze_text_chunks(text)
        return result['emotional_vector']


# UÅ»YCIE W AII:
from chunk_lexicon import ChunkLexicon

class AII:
    def __init__(self):
        self.chunk_lexicon = ChunkLexicon()  # NOWY!
        self.old_lexicon = EvolvingLexicon()  # MoÅ¼na zachowaÄ‡ dla kompatybilnoÅ›ci
    
    def interact(self, user_input):
        # Analiza CHUNKOWA (nowa)
        chunk_analysis = self.chunk_lexicon.analyze_text_chunks(user_input)
        
        # JeÅ›li pokrycie > 50%, uÅ¼ywamy chunkÃ³w
        if chunk_analysis['coverage'] > 0.5:
            vec = chunk_analysis['emotional_vector']
            print(f"[CHUNKS] Pokrycie: {chunk_analysis['coverage']:.0%}")
            print(f"[CHUNKS] Priming boost: {chunk_analysis['priming_boost']:.2f}x")
        else:
            # Fallback do starego leksykonu
            vec, _, _ = self.old_lexicon.analyze_text(user_input)
        
        # ... (reszta bez zmian)
'''
    print(example)


# =============================================================================
# TEST
# =============================================================================

def test_chunk_lexicon():
    """Test chunk-based lexicon."""
    print("\nðŸ§ª Test ChunkLexicon (Nature Human Behaviour 2026)\n")
    
    lexicon = ChunkLexicon(chunk_file="test_chunks.json")
    
    # Test 1: Ekstraktuj chunki z tekstu
    print("[Test 1] Automatyczna ekstrakcja chunkÃ³w")
    sample_text = """
    Czy mogÄ™ prosiÄ‡ o kawÄ™? Czy mogÄ™ prosiÄ‡ o herbatÄ™?
    W Å›rodku nocy zadzwoniÅ‚ telefon. W Å›rodku lasu znaleÅºliÅ›my chatÄ™.
    DziÄ™kujÄ™ bardzo za pomoc. DziÄ™kujÄ™ bardzo za wszystko.
    """
    
    lexicon.extract_chunks_from_text(sample_text)
    print(f"  Wyekstrahowano {len(lexicon.chunks)} chunkÃ³w")
    print(f"  NajczÄ™stsze: {lexicon.get_most_common_chunks(3)}")
    print("  âœ… PASS\n")
    
    # Test 2: Rozpoznawanie chunkÃ³w w nowym tekÅ›cie
    print("[Test 2] Rozpoznawanie chunkÃ³w (PRIMING)")
    new_text = "Czy mogÄ™ prosiÄ‡ o wodÄ™?"
    
    result = lexicon.analyze_text_chunks(new_text)
    print(f"  Tekst: '{new_text}'")
    print(f"  Znalezione chunki: {result['chunks_found']}")
    print(f"  Pokrycie: {result['coverage']:.0%}")
    print(f"  Priming boost: {result['priming_boost']:.2f}x")
    
    # Ponowne napotkanie -> silniejszy priming
    result2 = lexicon.analyze_text_chunks(new_text)
    print(f"\n  [Po ponownym kontakcie]")
    print(f"  Priming boost: {result2['priming_boost']:.2f}x (wzrost!)")
    print("  âœ… PASS\n")
    
    # Test 3: Uczenie emocji
    print("[Test 3] Uczenie emocji dla chunkÃ³w")
    chunk_text = "czy mogÄ™ prosiÄ‡ o"
    emotion_vec = np.zeros(15)
    emotion_vec[0] = 0.8  # RadoÅ›Ä‡
    emotion_vec[7] = 0.6  # Akceptacja
    
    lexicon.teach_chunk_emotion(chunk_text, emotion_vec, strength=0.8)
    chunk = lexicon.chunks[chunk_text]
    print(f"  Chunk: '{chunk_text}'")
    print(f"  Wektor emocjonalny: {chunk.emotional_vector[:8]}")
    print("  âœ… PASS\n")
    
    # Test 4: Statystyki
    print("[Test 4] Statystyki")
    stats = lexicon.get_statistics()
    print(f"  ChunkÃ³w: {stats['total_chunks']}")
    print(f"  Åšrednia czÄ™stoÅ›Ä‡: {stats['avg_frequency']:.1f}")
    print(f"  Åšrednia dÅ‚ugoÅ›Ä‡: {stats['avg_length']:.1f} sÅ‚Ã³w")
    print("  âœ… PASS\n")
    
    # Test 5: Save/Load
    print("[Test 5] Persistence")
    lexicon.save()
    
    lexicon2 = ChunkLexicon(chunk_file="test_chunks.json")
    assert len(lexicon2.chunks) == len(lexicon.chunks)
    print(f"  Wczytano {len(lexicon2.chunks)} chunkÃ³w")
    print("  âœ… PASS\n")
    
    # Cleanup
    import os
    if os.path.exists("test_chunks.json"):
        os.remove("test_chunks.json")
    
    print("âœ… Wszystkie testy przeszÅ‚y!\n")


if __name__ == "__main__":
    test_chunk_lexicon()
    
    print("\n" + "="*70)
    print("INTEGRACJA Z AII:")
    print("="*70)
    upgrade_aii_to_chunks()